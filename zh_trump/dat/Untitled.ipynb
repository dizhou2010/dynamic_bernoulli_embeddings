{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.load('d_emb_seg/raw/201704.txt.npy')\n",
    "array2 = np.load('d_emb_seg/train/201704.txt.npy')\n",
    "array3 = np.load('d_emb_seg/test/201704.txt.npy')\n",
    "array4 = np.load('d_emb_seg/valid/201704.txt.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(array.shape, array2.shape, array3.shape, array4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the name of the folder where your dataset is\n",
    "dataset_name = 'd_emb_seg'\n",
    "\n",
    "# Change this to a list of the time slices \n",
    "time_slices = pd.date_range('2014-12-01','2020-08-31', freq='MS').strftime(\"%Y%m\").tolist()\n",
    "\n",
    "#Change this to the number of characters in the file names that should be matched to the timeslice prefix.\n",
    "# i.e. if you use time_slices = [91, 92, 98, ...] \n",
    "#         use prefix_length = 2\n",
    "# if you use time_slices = [1998, 1999, 2000, 2001]\n",
    "#         use prefix_length = 4\n",
    "prefix_length = 6\n",
    "\n",
    "# Change this to a list of query words you would like the algorithm to print descriptive statistics of (i.e. a trajectory of the learned dynamic embeddings)\n",
    "query_words = ['美国', '中国', '自由', '民主', '特朗普']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify any code below\n",
    "#######################################################\n",
    "dat_stats={}\n",
    "dat_stats['name'] = dataset_name\n",
    "dat_stats['T_bins'] = time_slices\n",
    "dat_stats['prefix'] = prefix_length\n",
    "dat_stats['query_words'] = query_words\n",
    "T = len(dat_stats['T_bins'])\n",
    "\n",
    "def count_words(split):\n",
    "    dat_stats[split] = np.zeros(T)\n",
    "    files = glob.glob(dataset_name + '/'+ split + '/*.npy')\n",
    "    dat_files = []\n",
    "    for t, i in enumerate(dat_stats['T_bins']):\n",
    "        for f in files:\n",
    "            if os.path.basename(f)[:dat_stats['prefix']] == i:\n",
    "                dat_files.append(f)\n",
    "    for i in range(len(dat_stats['T_bins'])):\n",
    "        dat = np.load(dat_files[i])\n",
    "        dat_stats[split][i] += len(dat)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words2(split):\n",
    "    dat_stats[split] = np.zeros(T)\n",
    "    files = glob.glob(dataset_name + '/'+ split + '/*.npy')\n",
    "    for t, i in enumerate(dat_stats['T_bins']):\n",
    "        dat_files = [f for f in files if os.path.basename(f)[:dat_stats['prefix']] == i]\n",
    "        for fname in dat_files:\n",
    "            dat = np.load(fname)\n",
    "            dat_stats[split][t] += len(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26098.,    913.,   1191.,    459.,   2519.,   1945.,   9345.,\n",
       "         9371.,   2373.,   3622.,   4192.,   3050.,   5496.,   3368.,\n",
       "        22369.,  31793.,  20092.,  26199.,  29342.,  46664.,  22182.,\n",
       "        49930., 102572., 264905.,  38230.,  43454.,  29239.,  23578.,\n",
       "        21160.,  18883.,  36051.,  21249.,  36208.,  25144.,  38777.,\n",
       "        22488.,  47891.,  16388.,  21538.,  29155.,  39641.,  33472.,\n",
       "        25504.,  19932.,  18346.,  12004.,  13297.,  12971.,  10657.,\n",
       "        14447.,  12159.,  12167.,   7710.,  21387.,  23804.,  25138.,\n",
       "        30240.,  23078.,  14222.,  12859.,  30023.,  41858.,  19113.,\n",
       "       222842., 190281., 204147., 237355., 171097.,  95888.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_stats['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words2('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26098.,    913.,   1191.,    459.,   2519.,   1945.,   9345.,\n",
       "         9371.,   2373.,   3622.,   4192.,   3050.,   5496.,   3368.,\n",
       "        22369.,  31793.,  20092.,  26199.,  29342.,  46664.,  22182.,\n",
       "        49930., 102572., 264905.,  38230.,  43454.,  29239.,  23578.,\n",
       "        21160.,  18883.,  36051.,  21249.,  36208.,  25144.,  38777.,\n",
       "        22488.,  47891.,  16388.,  21538.,  29155.,  39641.,  33472.,\n",
       "        25504.,  19932.,  18346.,  12004.,  13297.,  12971.,  10657.,\n",
       "        14447.,  12159.,  12167.,   7710.,  21387.,  23804.,  25138.,\n",
       "        30240.,  23078.,  14222.,  12859.,  30023.,  41858.,  19113.,\n",
       "       222842., 190281., 204147., 237355., 171097.,  95888.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_stats['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_stats['train'] = np.zeros(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(dataset_name + '/'+ 'train' + '/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_files = []\n",
    "for t, i in enumerate(dat_stats['T_bins']):\n",
    "    print(t, i)\n",
    "    for f in files:\n",
    "        if os.path.basename(f)[:dat_stats['prefix']] == i:\n",
    "            dat_files.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dat_stats['T_bins'])):\n",
    "    dat = np.load(dat_files[i])\n",
    "    dat_stats['train'][i] += len(dat)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words('train')\n",
    "count_words('test')\n",
    "count_words('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dat_stats, open(dataset_name + '/dat_stats.pkl', \"ab+\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_name + '/dat_stats.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
