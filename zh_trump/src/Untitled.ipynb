{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bern_emb_data():\n",
    "    def __init__(self, cs, ns, fpath, dynamic, n_epochs=1, remove_stopwords=False):\n",
    "        assert cs%2 == 0\n",
    "        self.cs = cs\n",
    "        self.ns = ns\n",
    "        self.n_epochs = n_epochs\n",
    "        self.dynamic = dynamic\n",
    "        dat_stats = pickle.load(open(os.path.join(fpath, \"dat_stats.pkl\"), \"rb\"), encoding='latin1')\n",
    "        self.T = len(dat_stats['T_bins'])\n",
    "        self.name = dat_stats['name']\n",
    "        if not self.dynamic:\n",
    "            self.N = np.sum(dat_stats['train']).astype('int32')\n",
    "            self.n_train = (self.N/n_epochs).astype('int32')\n",
    "            self.n_valid = np.sum(dat_stats['valid']).astype('int32')\n",
    "            self.n_test = np.sum(dat_stats['test']).astype('int32')\n",
    "        else:\n",
    "            self.N = np.sum(dat_stats['train']).astype('int32')\n",
    "            self.n_train = np.maximum(dat_stats['train']/n_epochs, 2*np.ones_like(dat_stats['train'])).astype('int32')\n",
    "            self.n_valid = dat_stats['valid'].astype('int32')\n",
    "            self.n_test = dat_stats['test'].astype('int32')\n",
    "\n",
    "    # load vocabulary\n",
    "        df = pd.read_csv(os.path.join(fpath, 'unigram.txt'), delimiter='\\t',header=None)\n",
    "        self.labels = df[0].values\n",
    "        self.counts = df[len(df.columns)-1].values\n",
    "        counts = (1.0 * self.counts / self.N) ** (3.0 / 4)\n",
    "        self.unigram = counts / self.N\n",
    "        self.w_idx = range(len(self.labels))\n",
    "        if remove_stopwords:\n",
    "            sw_df = pd.read_csv(os.path.join(fpath, 'stop_words.txt'), delimiter='\\t',header=None)\n",
    "            stop_words = sw_df[0].values \n",
    "            self.w_idx = [i for i, w in enumerate(self.labels) if w not in stop_words]\n",
    "            self.labels = self.labels[self.w_idx]\n",
    "            self.counts = self.counts[self.w_idx]\n",
    "            self.unigram = self.unigram[self.w_idx]\n",
    "            self.unigram_t = np.load(os.path.join(fpath,'unigram_t.npy'))[:,self.w_idx]\n",
    "            self.unigram_t = self.unigram_t/self.unigram_t.sum(axis=0)\n",
    "        self.L = len(self.labels)\n",
    "        self.dictionary = dict(zip(self.labels,range(self.L)))\n",
    "        self.query_words = [w for w in dat_stats['query_words'] if w in self.labels]\n",
    "\n",
    "        # data generator (training)\n",
    "        train_files = glob.glob(os.path.join(fpath,'train','*.npy'))\n",
    "        if self.dynamic:\n",
    "            self.batch = [0]*self.T\n",
    "            for t, i in enumerate(dat_stats['T_bins']):\n",
    "                self.batch[t] = self.batch_generator(self.n_train[t] + self.cs, [f for f in train_files if os.path.basename(f)[:dat_stats['prefix']] == i])\n",
    "        else:\n",
    "            self.batch = self.batch_generator(self.n_train + self.cs, train_files)\n",
    "\n",
    "        # data generator (validation)\n",
    "        valid_files = glob.glob(os.path.join(fpath,'valid','*.npy'))\n",
    "        if self.dynamic:\n",
    "            self.valid_batch = [0]*self.T\n",
    "            for t, i in enumerate(dat_stats['T_bins']):\n",
    "                self.valid_batch[t] = self.batch_generator(self.n_valid[t] + self.cs, [f for f in valid_files if int(os.path.basename(f)[:dat_stats['prefix']]) == i])\n",
    "        else:\n",
    "            self.valid_batch = self.batch_generator(self.n_valid + self.cs, valid_files)\n",
    "\n",
    "        # data generator (test)\n",
    "        test_files = glob.glob(os.path.join(fpath,'test','*.npy'))\n",
    "        if self.dynamic:\n",
    "            self.test_batch = [0]*self.T\n",
    "            for t, i in enumerate(dat_stats['T_bins']):\n",
    "                self.test_batch[t] = self.batch_generator(self.n_test[t] + self.cs, [f for f in test_files if int(os.path.basename(f)[:dat_stats['prefix']]) == i])\n",
    "        else:\n",
    "            self.test_batch = self.batch_generator(self.n_test + self.cs, test_files)\n",
    "\n",
    "    def load_file(self, fn):\n",
    "        with open(fn, 'r') as myfile:\n",
    "            words = myfile.read().replace('\\n', '').split()\n",
    "        data = np.zeros(len(words))\n",
    "        for idx, word in enumerate(words):\n",
    "            if word in self.dictionary:\n",
    "                data[idx] = self.dictionary[word]\n",
    "        return data\n",
    "\n",
    "    def batch_generator(self, batch_size, files):\n",
    "        f_idx = 0\n",
    "        #data = self.load_file(files[f_idx])\n",
    "        data = np.load(files[f_idx])\n",
    "        while True:\n",
    "            if data.shape[0] < batch_size:\n",
    "                f_idx+=1\n",
    "                if (f_idx>=len(files)):\n",
    "                    f_idx = 0\n",
    "        \t#data_new = self.load_file(files[f_idx])\n",
    "                data_new = np.load(files[f_idx])\n",
    "                data = np.hstack([data, data_new])\n",
    "                if data.shape[0] < batch_size:\n",
    "                    continue\n",
    "            words = data[:batch_size]\n",
    "            data = data[batch_size:]\n",
    "            yield words\n",
    "    \n",
    "    def train_feed(self, placeholder):\n",
    "        if self.dynamic:\n",
    "            feed_dict = {}\n",
    "            for t in range(self.T):\n",
    "                feed_dict[placeholder[t]] = self.batch[t].__next__()\n",
    "            return feed_dict\n",
    "        else:\n",
    "            return {placeholder: self.batch.__next__()}\n",
    "\n",
    "    def valid_feed(self, placeholder):\n",
    "        if self.dynamic:\n",
    "            feed_dict = {}\n",
    "            for t in range(self.T):\n",
    "                feed_dict[placeholder[t]] = self.valid_batch[t].__next__()\n",
    "                return feed_dict\n",
    "        else:\n",
    "            return {placeholder: self.valid_batch.__next__()}\n",
    "\n",
    "    def test_feed(self, placeholder):\n",
    "        if self.dynamic:\n",
    "            feed_dict = {}\n",
    "            for t in range(self.T):\n",
    "                feed_dict[placeholder[t]] = self.test_batch[t].__next__()\n",
    "            return feed_dict\n",
    "        else:\n",
    "            return {placeholder: self.test_batch.__next__()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dat_stats = pickle.load(open(os.path.join(fpath, \"dat_stats.pkl\"), \"rb\"), encoding='latin1')\n",
    "fpath = '../dat/d_emb_seg'\n",
    "test_files = glob.glob(os.path.join(fpath,'test','*.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dat/d_emb_seg/test/201703.txt.npy',\n",
       " '../dat/d_emb_seg/test/201609.txt.npy',\n",
       " '../dat/d_emb_seg/test/201804.txt.npy']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_files[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, i in enumerate(dat_stats['T_bins']):\n",
    "    \n",
    "    [f for f in test_files if os.path.basename(f)[:dat_stats['prefix']] == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'201412'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_stats['T_bins'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(test_files[0])[:dat_stats['prefix']] == \"201703\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dat/d_emb_seg/test/201703.txt.npy',\n",
       " '../dat/d_emb_seg/test/201609.txt.npy',\n",
       " '../dat/d_emb_seg/test/201804.txt.npy',\n",
       " '../dat/d_emb_seg/test/201610.txt.npy',\n",
       " '../dat/d_emb_seg/test/201907.txt.npy',\n",
       " '../dat/d_emb_seg/test/201504.txt.npy',\n",
       " '../dat/d_emb_seg/test/202004.txt.npy',\n",
       " '../dat/d_emb_seg/test/201601.txt.npy',\n",
       " '../dat/d_emb_seg/test/201611.txt.npy',\n",
       " '../dat/d_emb_seg/test/201505.txt.npy',\n",
       " '../dat/d_emb_seg/test/201906.txt.npy',\n",
       " '../dat/d_emb_seg/test/202005.txt.npy',\n",
       " '../dat/d_emb_seg/test/201702.txt.npy',\n",
       " '../dat/d_emb_seg/test/201712.txt.npy',\n",
       " '../dat/d_emb_seg/test/201805.txt.npy',\n",
       " '../dat/d_emb_seg/test/201608.txt.npy',\n",
       " '../dat/d_emb_seg/test/201709.txt.npy',\n",
       " '../dat/d_emb_seg/test/201507.txt.npy',\n",
       " '../dat/d_emb_seg/test/201904.txt.npy',\n",
       " '../dat/d_emb_seg/test/201603.txt.npy',\n",
       " '../dat/d_emb_seg/test/202007.txt.npy',\n",
       " '../dat/d_emb_seg/test/201807.txt.npy',\n",
       " '../dat/d_emb_seg/test/201710.txt.npy',\n",
       " '../dat/d_emb_seg/test/201806.txt.npy',\n",
       " '../dat/d_emb_seg/test/201701.txt.npy',\n",
       " '../dat/d_emb_seg/test/201711.txt.npy',\n",
       " '../dat/d_emb_seg/test/201905.txt.npy',\n",
       " '../dat/d_emb_seg/test/201708.txt.npy',\n",
       " '../dat/d_emb_seg/test/201506.txt.npy',\n",
       " '../dat/d_emb_seg/test/201602.txt.npy',\n",
       " '../dat/d_emb_seg/test/201612.txt.npy',\n",
       " '../dat/d_emb_seg/test/202006.txt.npy',\n",
       " '../dat/d_emb_seg/test/202003.txt.npy',\n",
       " '../dat/d_emb_seg/test/201910.txt.npy',\n",
       " '../dat/d_emb_seg/test/201503.txt.npy',\n",
       " '../dat/d_emb_seg/test/201607.txt.npy',\n",
       " '../dat/d_emb_seg/test/201803.txt.npy',\n",
       " '../dat/d_emb_seg/test/201909.txt.npy',\n",
       " '../dat/d_emb_seg/test/201704.txt.npy',\n",
       " '../dat/d_emb_seg/test/201812.txt.npy',\n",
       " '../dat/d_emb_seg/test/201802.txt.npy',\n",
       " '../dat/d_emb_seg/test/201705.txt.npy',\n",
       " '../dat/d_emb_seg/test/201908.txt.npy',\n",
       " '../dat/d_emb_seg/test/202002.txt.npy',\n",
       " '../dat/d_emb_seg/test/201502.txt.npy',\n",
       " '../dat/d_emb_seg/test/201512.txt.npy',\n",
       " '../dat/d_emb_seg/test/201911.txt.npy',\n",
       " '../dat/d_emb_seg/test/201901.txt.npy',\n",
       " '../dat/d_emb_seg/test/201606.txt.npy',\n",
       " '../dat/d_emb_seg/test/201707.txt.npy',\n",
       " '../dat/d_emb_seg/test/201509.txt.npy',\n",
       " '../dat/d_emb_seg/test/201810.txt.npy',\n",
       " '../dat/d_emb_seg/test/201809.txt.npy',\n",
       " '../dat/d_emb_seg/test/201604.txt.npy',\n",
       " '../dat/d_emb_seg/test/201510.txt.npy',\n",
       " '../dat/d_emb_seg/test/201903.txt.npy',\n",
       " '../dat/d_emb_seg/test/202001.txt.npy',\n",
       " '../dat/d_emb_seg/test/201605.txt.npy',\n",
       " '../dat/d_emb_seg/test/201808.txt.npy',\n",
       " '../dat/d_emb_seg/test/201912.txt.npy',\n",
       " '../dat/d_emb_seg/test/201902.txt.npy',\n",
       " '../dat/d_emb_seg/test/201501.txt.npy',\n",
       " '../dat/d_emb_seg/test/201511.txt.npy',\n",
       " '../dat/d_emb_seg/test/202008.txt.npy',\n",
       " '../dat/d_emb_seg/test/201706.txt.npy',\n",
       " '../dat/d_emb_seg/test/201508.txt.npy',\n",
       " '../dat/d_emb_seg/test/201412.txt.npy',\n",
       " '../dat/d_emb_seg/test/201811.txt.npy',\n",
       " '../dat/d_emb_seg/test/201801.txt.npy']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f for f in test_files for t, i in enumerate(dat_stats['T_bins']) if os.path.basename(f)[:dat_stats['prefix']] == i ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dat/d_emb_seg/test/201412.txt.npy']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f for f in test_files if os.path.basename(f)[:dat_stats['prefix']] == '201412']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_file = []\n",
    "for t, i in enumerate(dat_stats['T_bins']):\n",
    "    for f in test_files:\n",
    "        if os.path.basename(f)[:dat_stats['prefix']] == i:\n",
    "            dat_file.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dat/d_emb_seg/test/201412.txt.npy',\n",
       " '../dat/d_emb_seg/test/201501.txt.npy',\n",
       " '../dat/d_emb_seg/test/201502.txt.npy',\n",
       " '../dat/d_emb_seg/test/201503.txt.npy',\n",
       " '../dat/d_emb_seg/test/201504.txt.npy',\n",
       " '../dat/d_emb_seg/test/201505.txt.npy',\n",
       " '../dat/d_emb_seg/test/201506.txt.npy',\n",
       " '../dat/d_emb_seg/test/201507.txt.npy',\n",
       " '../dat/d_emb_seg/test/201508.txt.npy',\n",
       " '../dat/d_emb_seg/test/201509.txt.npy',\n",
       " '../dat/d_emb_seg/test/201510.txt.npy',\n",
       " '../dat/d_emb_seg/test/201511.txt.npy',\n",
       " '../dat/d_emb_seg/test/201512.txt.npy',\n",
       " '../dat/d_emb_seg/test/201601.txt.npy',\n",
       " '../dat/d_emb_seg/test/201602.txt.npy',\n",
       " '../dat/d_emb_seg/test/201603.txt.npy',\n",
       " '../dat/d_emb_seg/test/201604.txt.npy',\n",
       " '../dat/d_emb_seg/test/201605.txt.npy',\n",
       " '../dat/d_emb_seg/test/201606.txt.npy',\n",
       " '../dat/d_emb_seg/test/201607.txt.npy',\n",
       " '../dat/d_emb_seg/test/201608.txt.npy',\n",
       " '../dat/d_emb_seg/test/201609.txt.npy',\n",
       " '../dat/d_emb_seg/test/201610.txt.npy',\n",
       " '../dat/d_emb_seg/test/201611.txt.npy',\n",
       " '../dat/d_emb_seg/test/201612.txt.npy',\n",
       " '../dat/d_emb_seg/test/201701.txt.npy',\n",
       " '../dat/d_emb_seg/test/201702.txt.npy',\n",
       " '../dat/d_emb_seg/test/201703.txt.npy',\n",
       " '../dat/d_emb_seg/test/201704.txt.npy',\n",
       " '../dat/d_emb_seg/test/201705.txt.npy',\n",
       " '../dat/d_emb_seg/test/201706.txt.npy',\n",
       " '../dat/d_emb_seg/test/201707.txt.npy',\n",
       " '../dat/d_emb_seg/test/201708.txt.npy',\n",
       " '../dat/d_emb_seg/test/201709.txt.npy',\n",
       " '../dat/d_emb_seg/test/201710.txt.npy',\n",
       " '../dat/d_emb_seg/test/201711.txt.npy',\n",
       " '../dat/d_emb_seg/test/201712.txt.npy',\n",
       " '../dat/d_emb_seg/test/201801.txt.npy',\n",
       " '../dat/d_emb_seg/test/201802.txt.npy',\n",
       " '../dat/d_emb_seg/test/201803.txt.npy',\n",
       " '../dat/d_emb_seg/test/201804.txt.npy',\n",
       " '../dat/d_emb_seg/test/201805.txt.npy',\n",
       " '../dat/d_emb_seg/test/201806.txt.npy',\n",
       " '../dat/d_emb_seg/test/201807.txt.npy',\n",
       " '../dat/d_emb_seg/test/201808.txt.npy',\n",
       " '../dat/d_emb_seg/test/201809.txt.npy',\n",
       " '../dat/d_emb_seg/test/201810.txt.npy',\n",
       " '../dat/d_emb_seg/test/201811.txt.npy',\n",
       " '../dat/d_emb_seg/test/201812.txt.npy',\n",
       " '../dat/d_emb_seg/test/201901.txt.npy',\n",
       " '../dat/d_emb_seg/test/201902.txt.npy',\n",
       " '../dat/d_emb_seg/test/201903.txt.npy',\n",
       " '../dat/d_emb_seg/test/201904.txt.npy',\n",
       " '../dat/d_emb_seg/test/201905.txt.npy',\n",
       " '../dat/d_emb_seg/test/201906.txt.npy',\n",
       " '../dat/d_emb_seg/test/201907.txt.npy',\n",
       " '../dat/d_emb_seg/test/201908.txt.npy',\n",
       " '../dat/d_emb_seg/test/201909.txt.npy',\n",
       " '../dat/d_emb_seg/test/201910.txt.npy',\n",
       " '../dat/d_emb_seg/test/201911.txt.npy',\n",
       " '../dat/d_emb_seg/test/201912.txt.npy',\n",
       " '../dat/d_emb_seg/test/202001.txt.npy',\n",
       " '../dat/d_emb_seg/test/202002.txt.npy',\n",
       " '../dat/d_emb_seg/test/202003.txt.npy',\n",
       " '../dat/d_emb_seg/test/202004.txt.npy',\n",
       " '../dat/d_emb_seg/test/202005.txt.npy',\n",
       " '../dat/d_emb_seg/test/202006.txt.npy',\n",
       " '../dat/d_emb_seg/test/202007.txt.npy',\n",
       " '../dat/d_emb_seg/test/202008.txt.npy']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_cs = int(4)\n",
    "args_ns = int(10)\n",
    "args_fpath = '../dat/d_emb_seg'\n",
    "args_dynamic = True\n",
    "args_n_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = bern_emb_data(args_cs, args_ns, args_fpath, args_dynamic, args_n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
